# Centralized Search Platform for Enterprise Automotive Marketplace

A production-grade, scalable search platform built with a monorepo architecture for enterprise automotive marketplace operations. This system supports millions of records with sub-second search responses, event-driven synchronization, and comprehensive role-based access control.

## ğŸ—ï¸ Architecture Overview

### System Design
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Offer Service  â”‚    â”‚Purchase Service â”‚    â”‚Transport Serviceâ”‚    â”‚ Search Service  â”‚
â”‚    (Port 3001)  â”‚    â”‚   (Port 3002)   â”‚    â”‚   (Port 3003)   â”‚    â”‚   (Port 3000)   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚PostgreSQL   â”‚ â”‚    â”‚ â”‚PostgreSQL   â”‚ â”‚    â”‚ â”‚PostgreSQL   â”‚ â”‚    â”‚ â”‚Elasticsearchâ”‚ â”‚
â”‚ â”‚offer_db     â”‚ â”‚    â”‚ â”‚purchase_db  â”‚ â”‚    â”‚ â”‚transport_db â”‚ â”‚    â”‚ â”‚+ Redis Cacheâ”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         Kafka Message Bus           â”‚
                    â”‚    (Event-Driven Architecture)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         React Frontends             â”‚
                    â”‚  UI: Port 3100  â”‚  UI-New: Port 3101â”‚
                    â”‚    (Docker)     â”‚   (Port 8080 dev) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

- **ğŸ” Advanced Search Engine**: Elasticsearch with fuzzy matching, autocomplete, synonyms, and edge n-grams
- **âš¡ High Performance**: Sub-second search responses for 10M+ records with Redis caching
- **ğŸ” Role-Based Access Control**: SELLER, BUYER, CARRIER, AGENT permissions with data isolation
- **ğŸ“¡ Event-Driven Architecture**: Kafka-based eventual consistency across microservices
- **ğŸ³ Container-Ready**: Full Docker Compose orchestration with health checks
- **ğŸ“Š Production Monitoring**: Health endpoints, metrics, and comprehensive error handling
- **ğŸ§ª Load Testing**: k6-based performance testing for concurrent operations
- **ğŸ¯ TypeScript**: Full type safety across backend services and React frontend

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Node.js 18+ (for development)
- Git

### 1. Clone and Start All Services

```bash
git clone <repository-url>
cd No-code-report

# Start all services with Docker Compose
docker-compose up -d

# Check service health
curl http://localhost:3000/health  # Search Service
curl http://localhost:3001/health  # Offer Service
curl http://localhost:3002/health  # Purchase Service
curl http://localhost:3003/health  # Transport Service
```

### 2. Access the Application

- **React UI (Original)**: http://localhost:3100
- **React UI (New)**: http://localhost:3101 (Docker) / http://localhost:8080 (Development)
- **Search API**: http://localhost:3000
- **Offer API**: http://localhost:3001 (Swagger: /api)
- **Purchase API**: http://localhost:3002 (Swagger: /api)
- **Transport API**: http://localhost:3003 (Swagger: /api)

### 3. Generate Test Data

```bash
cd scripts
npm install

# Quick start - generate default dataset
npm run generate                    # 50 users, 100 offers, 75 purchases, 30 transports

# Pre-configured dataset sizes
npm run generate:small              # ~100 total records
npm run generate:medium             # ~1,000 total records  
npm run generate:large              # ~5,000 total records
npm run generate:massive            # ~10,000 total records

# Custom amounts
node master-data-generator.js --users 200 --offers 500 --purchases 300

# Proportional scaling
node master-data-generator.js --all 50000     # 25K users, 50K offers, etc.
node master-data-generator.js --scale 10      # 10x default amounts

# Help and options
node master-data-generator.js --help
```

### 4. Run Performance Tests

```bash
cd scripts/load-testing
npm install

# Search-focused load testing
npm run search-test

# API services load testing
npm run api-test

# Mixed concurrent operations (production simulation)
npm run mixed-workload
```

## ğŸ“‹ Services Documentation

### Search Service (Port 3000)

**Centralized search with role-based filtering and caching**

#### Key Endpoints

```bash
# Universal Search
POST /search
{
  "userType": "BUYER|SELLER|CARRIER|AGENT",
  "accountId": "user-uuid",
  "searchText": "Toyota Camry",
  "entityTypes": ["offer", "purchase", "transport"],
  "minYear": 2020,
  "maxPrice": 50000,
  "page": 1,
  "limit": 20
}

# Role-Specific Search
POST /search/buyer
POST /search/seller
POST /search/carrier
POST /search/agent

# Autocomplete with Suggestions
GET /search/autocomplete?searchText=Toy&limit=10

# Search Statistics
GET /search/statistics
```

#### Advanced Features

- **Fuzzy Matching**: Handles typos and variations (e.g., "Toyoya" â†’ "Toyota")
- **Synonym Support**: "car" matches "vehicle", "automobile"
- **Edge N-grams**: Partial matching for autocomplete
- **Role-Based Filtering**: Users only see relevant data based on permissions
- **Redis Caching**: Frequently accessed results cached for performance
- **Aggregations**: Count by make, year, condition, status

### Offer Service (Port 3001)

**Vehicle offer management with comprehensive CRUD operations**

```bash
# Create Offer
POST /offers
{
  "sellerId": "uuid",
  "vin": "1HGBH41JXMN109186",
  "make": "Toyota",
  "model": "Camry LE",
  "year": 2022,
  "price": 28500,
  "condition": "NEW",
  "status": "ACTIVE",
  "description": "Brand new Toyota Camry with advanced safety features",
  "location": "New York, NY"
}

# Get Offers with Pagination
GET /offers?page=1&limit=20&sellerId=uuid

# Update Offer
PUT /offers/:id

# Offer Statistics
GET /offers/statistics
```

### Purchase Service (Port 3002)

**Purchase transaction management with offer validation**

```bash
# Create Purchase
POST /purchases
{
  "buyerId": "uuid",
  "offerId": "offer-uuid",
  "totalAmount": 28500,
  "paymentMethod": "FINANCING",
  "status": "PENDING"
}

# Get Purchase Details
GET /purchases/:id

# Purchase Statistics
GET /purchases/statistics
```

### Transport Service (Port 3003)

**Vehicle transport logistics with tracking**

```bash
# Create Transport Job
POST /transports
{
  "carrierId": "uuid",
  "purchaseId": "purchase-uuid",
  "pickupLocation": "New York, NY",
  "deliveryLocation": "Boston, MA",
  "scheduledPickup": "2024-01-15T10:00:00Z",
  "scheduledDelivery": "2024-01-17T15:00:00Z",
  "status": "SCHEDULED"
}

# Track Transport
GET /transports/:id

# Transport Statistics
GET /transports/statistics
```

## ğŸ”§ Development

### Local Development Setup

```bash
# Install dependencies for all services
npm run install-all

# Start services individually for development
cd services/search-service && npm run start:dev
cd services/offer-service && npm run start:dev
cd services/purchase-service && npm run start:dev
cd services/transport-service && npm run start:dev

# Start React UIs
cd ui && npm run dev          # Original UI (port 3100)
cd ui-new && npm run dev      # New UI (port 8080)

#to see elastic search
http://localhost:1358/?appname=*&url=http%3A%2F%2Flocalhost%3A9200&mode=edit&results=1


**pgAdmin Database Connections:**

| Table      | Host         | Port | Database      | Username | Password  |
|------------|--------------|------|--------------|----------|-----------|
| PURCHASES  | purchase-db  | 5432 | purchase_db  | postgres | password  |
| OFFERS     | offer-db     | 5432 | offer_db     | postgres | password  |
| TRANSPORTS | transport-db | 5432 | transport_db | postgres | password  |
| USERS      | user-db      | 5432 | user_db      | postgres | password  |

> **Tip:** In pgAdmin, create a new connection for each database using the details above to access the respective tables.


```

### Environment Variables

Create `.env` files in each service directory:

```bash
# Database Configuration
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=service_db
DATABASE_USERNAME=postgres
DATABASE_PASSWORD=postgres

# Kafka Configuration
KAFKA_BROKER=localhost:9092

# Redis Configuration (Search Service)
REDIS_URL=redis://localhost:6379

# Elasticsearch Configuration (Search Service)
ELASTICSEARCH_NODE=http://localhost:9200
```

## ğŸ§ª Testing

### Unit & Integration Tests

```bash
# Run tests for all services
npm run test:all

# Run tests for specific service
cd services/search-service && npm test
cd services/offer-service && npm test
```

### Load Testing Results

The system has been tested to handle:

- **Search Operations**: 100+ concurrent users with sub-second response times
- **API Operations**: 50+ concurrent users with <3s response times
- **Mixed Workloads**: Concurrent search + indexing operations
- **Data Scale**: 10M+ records with maintained performance

#### Sample k6 Results
```
âœ“ search status is 200.........................: 100.00% âœ“ 15234 âœ— 0
âœ“ search response time < 2s....................: 98.20%  âœ“ 14960 âœ— 274
âœ“ autocomplete response time < 500ms............: 99.80%  âœ“ 4990  âœ— 10
âœ“ concurrent operations success.................: 97.50%  âœ“ 1950  âœ— 50

http_req_duration...............: avg=850ms  min=45ms  med=620ms  max=4.2s  p(90)=1.5s  p(95)=2.1s
search_response_time............: avg=650ms  min=45ms  med=480ms  max=3.8s  p(90)=1.2s  p(95)=1.7s
```

## ğŸ­ Production Deployment

### Docker Compose Production

```bash
# Production deployment
docker-compose -f docker-compose.prod.yml up -d

# Scale services based on load
docker-compose up -d --scale search-service=3 --scale offer-service=2
```

### Kubernetes Deployment

Kubernetes manifests are available in `/k8s` directory:

```bash
kubectl apply -f k8s/namespace.yml
kubectl apply -f k8s/configmaps/
kubectl apply -f k8s/services/
kubectl apply -f k8s/deployments/
```

### Monitoring & Observability

- **Health Checks**: All services expose `/health` endpoints
- **Metrics**: Integration ready for Prometheus/Grafana
- **Logging**: Structured JSON logging for centralized collection
- **Tracing**: OpenTelemetry integration points available

## ğŸ“Š Performance Optimization

### Search Performance

- **Elasticsearch Optimization**:
  - Custom mapping with edge n-grams for autocomplete
  - Synonym filters for better matching
  - Fuzzy queries with controlled fuzziness
  - Field boosting for relevance tuning

- **Caching Strategy**:
  - Redis for frequently accessed search results
  - TTL-based cache invalidation
  - Cache warming for popular queries

### Database Performance

- **Indexing Strategy**:
  - Primary keys, foreign keys, and search-relevant fields
  - Composite indexes for common query patterns
  - Partial indexes for status-based queries

- **Connection Pooling**:
  - Configured connection pools for each service
  - Health check queries for connection validation

## ğŸ” Security & Access Control

### Role-Based Permissions

- **SELLER**: Can create/manage offers, view own purchases as sales
- **BUYER**: Can search offers, create purchases, view own transactions
- **CARRIER**: Can view transport jobs, update transport status
- **AGENT**: Administrative access to all entities with restrictions

### Data Isolation

- Row-level security through role-based filtering
- Elasticsearch queries automatically filtered by user permissions
- Account-specific data access validation

## ğŸ› Troubleshooting

### Common Issues

**Services Not Starting**
```bash
# Check Docker containers
docker-compose ps

# Check service logs
docker-compose logs search-service
docker-compose logs offer-service
```

**Search Not Working**
```bash
# Check Elasticsearch health
curl http://localhost:9200/_cluster/health

# Check search service logs
docker-compose logs search-service

# Verify index exists
curl http://localhost:9200/_cat/indices
```

**Event Processing Issues**
```bash
# Check Kafka topics
docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092

# Check consumer group status
docker-compose exec kafka kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group search-service-group
```

### Performance Issues

- **Slow Searches**: Check Elasticsearch query performance and consider index optimization
- **High Memory Usage**: Tune JVM settings for Elasticsearch and Kafka
- **Database Connections**: Monitor connection pool usage and tune settings

## ğŸ“ˆ Scaling Considerations

### Horizontal Scaling

- **Search Service**: Scale based on search volume (CPU-bound)
- **API Services**: Scale based on transaction volume
- **Database**: Consider read replicas for heavy read workloads
- **Elasticsearch**: Add nodes for index size and query performance

### Vertical Scaling

- **Memory**: Increase for Elasticsearch and Redis cache
- **CPU**: Scale for compute-intensive search operations
- **Storage**: SSD recommended for database and Elasticsearch

## ğŸ¤ Contributing

### Development Workflow

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

### Code Standards

- **TypeScript**: Strict type checking enabled
- **ESLint**: Configured for consistency
- **Prettier**: Code formatting
- **Testing**: Unit tests required for new features
- **Documentation**: Update README for significant changes

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™‹â€â™‚ï¸ Support

For questions, issues, or contributions:

- ğŸ“§ Create an issue in the repository
- ğŸ’¬ Join our development discussions
- ğŸ“– Check the documentation in `/docs`

---

**Built with â¤ï¸ for enterprise automotive marketplace needs**